{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["IlLBPSmAfEE0"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["```\n","note:\n","\n","using the table of contents panel makes this notebook easier to navigate\n","all of my written analysis is done in the \"Write Up\" section of the resepctive portion of the project\n","```"],"metadata":{"id":"HoLQlEO9TXtA"}},{"cell_type":"markdown","source":["# Part 1"],"metadata":{"id":"dybqJh_qJZzZ"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"mKwaLwQStmZj"}},{"cell_type":"code","source":["from __future__ import print_function\n","import argparse\n","import os\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torch.utils.tensorboard import SummaryWriter\n","# from ConvNet import ConvNet \n","import argparse\n","import numpy as np "],"metadata":{"id":"SDZPWqwQsNd9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"m9axjN6S34am"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ConvNet"],"metadata":{"id":"DbejC84Ntglp"}},{"cell_type":"markdown","source":["create the three models"],"metadata":{"id":"NOL6W_IH94fi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6hh3wrhpzhh"},"outputs":[],"source":["class ConvNet(nn.Module):\n","    def __init__(self, mode):\n","        super(ConvNet, self).__init__()\n","\n","        #model 0\n","        self.m0_fc1 = nn.Linear(28*28, 100).to(device)\n","        self.m0_fc2 = nn.Linear(100, 100).to(device)\n","        self.m0_fc3 = nn.Linear(100, 10).to(device)\n","\n","        #model 1\n","        self.m1_fc1 = nn.Linear(28*28, 20).to(device) # 20 neurons\n","        self.m1_fc2 = nn.Linear(20, 20).to(device) # 20 neurons\n","        self.m1_fc3 = nn.Linear(20, 10).to(device) # 10 for MINST\n","\n","        #model 2\n","        self.m2_fc1 = nn.Linear(28*28, 20).to(device) # 20 neurons\n","        self.m2_fc2 = nn.Linear(20, 20).to(device) # 20 neurons\n","        self.m2_fc3 = nn.Linear(20, 10).to(device) # 10 for MINST\n","\n","        #model 3\n","        self.m3_fc1 = nn.Linear(28*28, 2048).to(device)\n","        self.m3_fc2 = nn.Linear(2048, 1024).to(device)\n","        self.m3_fc3 = nn.Linear(1024, 512).to(device)\n","        self.m3_fc4 = nn.Linear(512, 256).to(device)\n","        self.m3_fc5 = nn.Linear(256, 10).to(device)      \n","        # self.m3_fc1 = nn.Linear(28*28, 1024).to(device)\n","        # self.m3_fc2 = nn.Linear(1024, 512).to(device)\n","        # self.m3_fc3 = nn.Linear(512, 256).to(device)\n","        # self.m3_fc4 = nn.Linear(256, 128).to(device)\n","        # self.m3_fc5 = nn.Linear(128, 64).to(device)      \n","        # self.m3_fc6 = nn.Linear(64, 32).to(device)\n","        # self.m3_fc7 = nn.Linear(32, 16).to(device)\n","        # self.m3_fc8 = nn.Linear(16, 10).to(device)\n","             \n","        # This will select the forward pass function based on mode for the ConvNet.\n","        # During creation of each ConvNet model, you will assign one of the valid mode.\n","        # This will fix the forward function (and the network graph) for the entire training/testing\n","        if mode == 1:\n","            self.forward = self.model_1\n","        elif mode == 2:\n","            self.forward = self.model_2\n","        elif mode == 3:\n","            self.forward = self.model_3\n","        else: \n","            print(\"Invalid mode \", mode, \"selected. Select between 1-3\")\n","            exit(0)\n","      \n","    # Baseline sample model\n","    def model_0(self, X):\n","        # ======================================================================\n","        # Three fully connected layers with activation\n","        \n","        X = torch.flatten(X, start_dim=1)\n","        X = self.m0_fc1(X)\n","        X = F.relu(X)\n","        X = self.m0_fc2(X)\n","        X = F.relu(X)\n","        X = self.m0_fc3(X)\n","        X = torch.sigmoid(X)\n","                \n","        return X  \n","        \n","    # Baseline model. task 1\n","    def model_1(self, X):\n","        # ======================================================================\n","        # Three fully connected layers without activation\n","\n","        # print('1 ')\n","\n","        X = X.to(device)\n","        X = torch.flatten(X, start_dim=1)\n","\n","        X = self.m1_fc1(X)\n","        X = self.m1_fc2(X)\n","        X = self.m1_fc3(X)\n","\n","        return X\n","        \n","\n","    # task 2\n","    def model_2(self, X):\n","        # ======================================================================\n","        # Train with activation (use model 1 from task 1)\n","\n","        # print('2 ')\n","        \n","        X = X.to(device)\n","        X = torch.flatten(X, start_dim=1)\n","\n","        X = self.m2_fc1(X)\n","        X = F.relu(X)\n","        X = self.m2_fc2(X)\n","        X = F.relu(X)\n","        X = self.m2_fc3(X)\n","        X = F.relu(X)\n","\n","        # X = torch.sigmoid(X) # i don't think i can use this cause the instructions say to only use RELU activation\n","        \n","        return X\n","\n","\t\n","    # task 3\n","    def model_3(self, X):\n","        # ======================================================================\n","        # Change number of fully connected layers and number of neurons from model 2 in task 2\n","\n","        # print('3 ')\n","        \n","        X = X.to(device)\n","        X = torch.flatten(X, start_dim=1)\n","\n","        X = self.m3_fc1(X)\n","        X = F.relu(X)\n","        X = self.m3_fc2(X)\n","        X = F.relu(X)\n","        X = self.m3_fc3(X)\n","        X = F.relu(X)\n","        X = self.m3_fc4(X)\n","        X = F.relu(X)\n","        X = self.m3_fc5(X)\n","        # these are old layers made before i realized we needed >200 neurons\n","        # X = F.relu(X)\n","        # X = self.m3_fc6(X)\n","        # X = F.relu(X)\n","        # X = self.m3_fc7(X)\n","        # X = F.relu(X)\n","        # X = self.m3_fc8(X)\n","\n","        X = F.relu(X) # i think i have to use this?\n","        # X = torch.sigmoid(X) # commented out b/c i wanted to try softmax\n","        # X = F.log_softmax(X, dim=1)\n","        \n","        return X\n","        "]},{"cell_type":"markdown","source":["## Helpers"],"metadata":{"id":"s70pB_5ctjb-"}},{"cell_type":"markdown","source":["these functions were provided by the professor, and weren't modified a lot"],"metadata":{"id":"I0qjAmMX9uqH"}},{"cell_type":"markdown","source":["### Train"],"metadata":{"id":"D0hUSHo-tnsB"}},{"cell_type":"code","source":["def train(model, device, train_loader, optimizer, criterion, epoch, batch_size):\n","    '''\n","    Trains the model for an epoch and optimizes it.\n","    model: The model to train. Should already be in correct device.\n","    device: 'cuda' or 'cpu'.\n","    train_loader: dataloader for training samples.\n","    optimizer: optimizer to use for model parameter updates.\n","    criterion: used to compute loss for prediction and target \n","    epoch: Current epoch to train for.\n","    batch_size: Batch size to be used.\n","    '''\n","    \n","    # Set model to train mode before each epoch\n","    model.train()\n","    \n","    # Empty list to store losses \n","    losses = []\n","    correct = 0\n","    \n","    # Iterate over entire training samples (1 epoch)\n","    for batch_idx, batch_sample in enumerate(train_loader):\n","        data, target = batch_sample\n","        \n","        # Push data/label to correct device\n","        data, target = data.to(device), target.to(device)\n","        \n","        # Reset optimizer gradients. Avoids grad accumulation (accumulation used in RNN).\n","        optimizer.zero_grad()\n","        \n","        # Do forward pass for current set of data\n","        output = model(data)\n","        \n","        # ======================================================================\n","        # Compute loss based on criterion\n","        # ----------------- YOUR CODE HERE ----------------------\n","        loss = criterion(output, target)\n","        \n","        # Computes gradient based on final loss\n","        loss.backward()\n","        \n","        # Store loss\n","        losses.append(loss.item())\n","        \n","        # Optimize model parameters based on learning rate and gradient \n","        optimizer.step()\n","        \n","        # Get predicted index by selecting maximum log-probability\n","        pred = output.argmax(dim=1, keepdim=True)\n","        \n","        # ======================================================================\n","        # Count correct predictions overall \n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","        \n","    train_loss = float(np.mean(losses))\n","    train_acc = correct / ((batch_idx+1) * batch_size)\n","    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n","        float(np.mean(losses)), correct, (batch_idx+1) * batch_size,\n","        100. * correct / ((batch_idx+1) * batch_size)))\n","    return train_loss, train_acc\n","    "],"metadata":{"id":"jraQIYJBp4eC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test"],"metadata":{"id":"8JzZxwZCtqGU"}},{"cell_type":"code","source":["def test(model, device, test_loader):\n","    '''\n","    Tests the model.\n","    model: The model to train. Should already be in correct device.\n","    device: 'cuda' or 'cpu'.\n","    test_loader: dataloader for test samples.\n","    '''\n","    \n","    # Set model to eval mode to notify all layers.\n","    model.eval()\n","    \n","    losses = []\n","    correct = 0\n","    \n","    # Set torch.no_grad() to disable gradient computation and backpropagation\n","    with torch.no_grad():\n","        for batch_idx, sample in enumerate(test_loader):\n","            data, target = sample\n","            data, target = data.to(device), target.to(device)\n","            \n","\n","            # Predict for data by doing forward pass\n","            output = model(data)\n","            \n","            # ======================================================================\n","            # Compute loss based on same criterion as training\n","            loss = F.cross_entropy(output, target, reduction='mean')\n","            \n","            # Append loss to overall test loss\n","            losses.append(loss.item())\n","            \n","            # Get predicted index by selecting maximum log-probability\n","            pred = output.argmax(dim=1, keepdim=True)\n","            \n","            # ======================================================================\n","            # Count correct predictions overall \n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss = float(np.mean(losses))\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","\n","    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n","        test_loss, correct, len(test_loader.dataset), accuracy))\n","    \n","    return test_loss, accuracy"],"metadata":{"id":"BMfCCXTbsLQx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main"],"metadata":{"id":"we9UvuxWtr0k"}},{"cell_type":"code","source":["def run_main(FLAGS):\n","    # # Check if cuda is available\n","    # use_cuda = torch.cuda.is_available()\n","    \n","    # # Set proper device based on cuda availability \n","    # # device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"Torch device selected: \", device)\n","    \n","    # Initialize the model and send to device \n","    model = ConvNet(FLAGS.mode).to(device)\n","    \n","    # Initialize the criterion for loss computation \n","    # ======================================================================\n","    criterion = torch.nn.CrossEntropyLoss()\n","    \n","    # Initialize optimizer type \n","    optimizer = optim.SGD(model.parameters(), lr=FLAGS.learning_rate, weight_decay=1e-7)\n","    \n","    # Create transformations to apply to each data sample \n","    # Can specify variations such as image flip, color flip, random crop, ...\n","    transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","    \n","    # Load datasets for training and testing\n","    # Inbuilt datasets available in torchvision (check documentation online)\n","    dataset1 = datasets.MNIST('./data/', train=True, download=True,\n","                       transform=transform)\n","    dataset2 = datasets.MNIST('./data/', train=False,\n","                       transform=transform)\n","    train_loader = DataLoader(dataset1, batch_size = FLAGS.batch_size, \n","                                shuffle=True, num_workers=2)#4)\n","    test_loader = DataLoader(dataset2, batch_size = FLAGS.batch_size, \n","                                shuffle=False, num_workers=2)#4)\n","    \n","    best_accuracy = 0.0\n","    \n","    # Run training for n_epochs specified in config \n","    for epoch in range(1, FLAGS.num_epochs + 1):\n","        print(\"\\nEpoch: \", epoch)\n","        train_loss, train_accuracy = train(model, device, train_loader,\n","                                            optimizer, criterion, epoch, FLAGS.batch_size)\n","        \n","        test_loss, test_accuracy = test(model, device, test_loader)\n","        \n","        if test_accuracy > best_accuracy:\n","            best_accuracy = test_accuracy\n","            \n","    print(\"\\nBest accuracy is {:2.2f}\".format(best_accuracy))\n","    \n","    print(\"\\nTraining and evaluation finished\")"],"metadata":{"id":"T7N3NzEMsG2w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RUN"],"metadata":{"id":"yK6s3hC_6GKM"}},{"cell_type":"markdown","source":["here we run the 3 models we created throughout completing the tasks"],"metadata":{"id":"y1-0zHnd9lEx"}},{"cell_type":"markdown","source":["### Model 1"],"metadata":{"id":"qQMXGIuO5nkG"}},{"cell_type":"markdown","source":["#### Code"],"metadata":{"id":"lTtDtHNqKN87"}},{"cell_type":"code","source":["# Set parameters for Sparse Autoencoder\n","parser = argparse.ArgumentParser('CNN Exercise.')\n","parser.add_argument('--mode',\n","                    type=int, default=1,\n","                    help='Select mode between 1-3.')\n","parser.add_argument('--learning_rate',\n","                    type=float, default=0.1,\n","                    help='Initial learning rate.')\n","parser.add_argument('--num_epochs',\n","                    type=int,\n","                    default=5,\n","                    help='Number of epochs to run trainer.')\n","parser.add_argument('--batch_size',\n","                    type=int, default=10,\n","                    help='Batch size. Must divide evenly into the dataset sizes.')\n","parser.add_argument('--log_dir',\n","                    type=str,\n","                    default='logs',\n","                    help='Directory to put logging.')\n","                    \n","FLAGS = None\n","FLAGS, unparsed = parser.parse_known_args()\n","\n","print(\"Mode: \", FLAGS.mode)\n","print(\"LR: \", FLAGS.learning_rate)\n","print(\"Batch size: \", FLAGS.batch_size)\n","\n","start_time = time.time()\n","run_main(FLAGS) \n","end_time = time.time()\n","\n","print(f'\\nTook: {end_time - start_time: .2f} seconds')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13XCYOG3sEi3","outputId":"136368ef-ec8c-455d-949c-eb8532ee0ece","executionInfo":{"status":"ok","timestamp":1682291506128,"user_tz":240,"elapsed":173272,"user":{"displayName":"Jan Darge","userId":"00143714718901829372"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mode:  1\n","LR:  0.1\n","Batch size:  10\n","Torch device selected:  cuda:0\n","Files already downloaded and verified\n","\n","Epoch:  1\n","Train set: Average loss: 1.6339, Accuracy: 20350/50000 (41%)\n","Test set: Average loss: 1.4817, Accuracy: 4646/10000 (46%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.4414, Accuracy: 24399/50000 (49%)\n","Test set: Average loss: 1.3998, Accuracy: 5113/10000 (51%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.3860, Accuracy: 25677/50000 (51%)\n","Test set: Average loss: 1.7159, Accuracy: 4188/10000 (42%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.3422, Accuracy: 26505/50000 (53%)\n","Test set: Average loss: 1.4264, Accuracy: 5198/10000 (52%)\n","\n","Epoch:  5\n","Train set: Average loss: 1.3262, Accuracy: 26880/50000 (54%)\n","Test set: Average loss: 1.3930, Accuracy: 5275/10000 (53%)\n","\n","Best accuracy is 52.75\n","\n","Training and evaluation finished\n","\n","Took:  173.29 seconds\n"]}]},{"cell_type":"markdown","source":["#### Previous Log"],"metadata":{"id":"cdsjMF0bKI3t"}},{"cell_type":"markdown","source":["```\n","Mode:  1\n","LR:  0.1\n","Batch size:  10\n","Torch device selected:  cuda:0\n","\n","Epoch:  1\n","Train set: Average loss: nan, Accuracy: 27545/60000 (46%)\n","Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n","\n","Epoch:  2\n","Train set: Average loss: nan, Accuracy: 5923/60000 (10%)\n","Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n","\n","Epoch:  3\n","Train set: Average loss: nan, Accuracy: 5923/60000 (10%)\n","Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n","\n","Epoch:  4\n","Train set: Average loss: nan, Accuracy: 5923/60000 (10%)\n","Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n","\n","Epoch:  5\n","Train set: Average loss: nan, Accuracy: 5923/60000 (10%)\n","Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n","\n","Best accuracy is 9.80\n","\n","Training and evaluation finished\n","\n","Took:  174.70 seconds\n","```"],"metadata":{"id":"rryEx_KAKSBg"}},{"cell_type":"markdown","source":["### Model 2"],"metadata":{"id":"7i8pdFjm5lMF"}},{"cell_type":"markdown","source":["#### Code"],"metadata":{"id":"Nc-9vl7BKT58"}},{"cell_type":"code","source":["# Set parameters for Sparse Autoencoder\n","parser = argparse.ArgumentParser('CNN Exercise.')\n","parser.add_argument('--mode',\n","                    type=int, default=2,\n","                    help='Select mode between 1-3.')\n","parser.add_argument('--learning_rate',\n","                    type=float, default=0.1,\n","                    help='Initial learning rate.')\n","parser.add_argument('--num_epochs',\n","                    type=int,\n","                    default=5,\n","                    help='Number of epochs to run trainer.')\n","parser.add_argument('--batch_size',\n","                    type=int, default=10,\n","                    help='Batch size. Must divide evenly into the dataset sizes.')\n","parser.add_argument('--log_dir',\n","                    type=str,\n","                    default='logs',\n","                    help='Directory to put logging.')\n","                    \n","FLAGS = None\n","FLAGS, unparsed = parser.parse_known_args()\n","\n","print(\"Mode: \", FLAGS.mode)\n","print(\"LR: \", FLAGS.learning_rate)\n","print(\"Batch size: \", FLAGS.batch_size)\n","\n","start_time = time.time()\n","run_main(FLAGS)\n","end_time = time.time()\n","\n","print(f'\\nTook: {end_time - start_time: .2f} seconds')"],"metadata":{"id":"cIKBFXah2ISy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e01c354-d915-471e-ccf7-080419b9de75","executionInfo":{"status":"ok","timestamp":1682290244969,"user_tz":240,"elapsed":186593,"user":{"displayName":"Jan Darge","userId":"00143714718901829372"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mode:  2\n","LR:  0.1\n","Batch size:  10\n","Torch device selected:  cuda:0\n","\n","Epoch:  1\n","Train set: Average loss: 0.3499, Accuracy: 53868/60000 (90%)\n","Test set: Average loss: 0.4477, Accuracy: 8688/10000 (87%)\n","\n","Epoch:  2\n","Train set: Average loss: 0.2420, Accuracy: 55997/60000 (93%)\n","Test set: Average loss: 0.2065, Accuracy: 9435/10000 (94%)\n","\n","Epoch:  3\n","Train set: Average loss: 0.2137, Accuracy: 56558/60000 (94%)\n","Test set: Average loss: 0.2099, Accuracy: 9420/10000 (94%)\n","\n","Epoch:  4\n","Train set: Average loss: 0.2033, Accuracy: 56711/60000 (95%)\n","Test set: Average loss: 0.1685, Accuracy: 9551/10000 (96%)\n","\n","Epoch:  5\n","Train set: Average loss: 0.1969, Accuracy: 56838/60000 (95%)\n","Test set: Average loss: 0.2199, Accuracy: 9464/10000 (95%)\n","\n","Best accuracy is 95.51\n","\n","Training and evaluation finished\n","\n","Took:  186.33 seconds\n"]}]},{"cell_type":"markdown","source":["#### Previous Log"],"metadata":{"id":"3JKWFy0MKWmT"}},{"cell_type":"markdown","source":["```\n","Mode:  2\n","LR:  0.1\n","Batch size:  10\n","Torch device selected:  cuda:0\n","\n","Epoch:  1\n","Train set: Average loss: 1.5871, Accuracy: 50087/60000 (83%)\n","Test set: Average loss: 1.5339, Accuracy: 9150/10000 (92%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.5242, Accuracy: 55597/60000 (93%)\n","Test set: Average loss: 1.5138, Accuracy: 9394/10000 (94%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.5130, Accuracy: 56345/60000 (94%)\n","Test set: Average loss: 1.5082, Accuracy: 9433/10000 (94%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.5076, Accuracy: 56647/60000 (94%)\n","Test set: Average loss: 1.5094, Accuracy: 9441/10000 (94%)\n","\n","Epoch:  5\n","Train set: Average loss: 1.5045, Accuracy: 56895/60000 (95%)\n","Test set: Average loss: 1.5052, Accuracy: 9471/10000 (95%)\n","\n","Best accuracy is 94.71\n","\n","Training and evaluation finished\n","\n","Took:  178.59 seconds\n","```"],"metadata":{"id":"1UPFIaDtKnNg"}},{"cell_type":"markdown","source":["### Model 3"],"metadata":{"id":"fXjx3fyz5pba"}},{"cell_type":"markdown","source":["#### Code"],"metadata":{"id":"nGv-caggKYwl"}},{"cell_type":"code","source":["# Set parameters for Sparse Autoencoder\n","parser = argparse.ArgumentParser('CNN Exercise.')\n","parser.add_argument('--mode',\n","                    type=int, default=3,\n","                    help='Select mode between 1-3.')\n","parser.add_argument('--learning_rate',\n","                    type=float, default=0.1,\n","                    help='Initial learning rate.')\n","parser.add_argument('--num_epochs',\n","                    type=int,\n","                    default=5,\n","                    help='Number of epochs to run trainer.')\n","parser.add_argument('--batch_size',\n","                    type=int, default=10,\n","                    help='Batch size. Must divide evenly into the dataset sizes.')\n","parser.add_argument('--log_dir',\n","                    type=str,\n","                    default='logs',\n","                    help='Directory to put logging.')\n","                    \n","FLAGS = None\n","FLAGS, unparsed = parser.parse_known_args()\n","\n","print(\"Mode: \", FLAGS.mode)\n","print(\"LR: \", FLAGS.learning_rate)\n","print(\"Batch size: \", FLAGS.batch_size)\n","\n","start_time = time.time()\n","run_main(FLAGS)\n","end_time = time.time()\n","\n","print(f'\\nTook: {end_time - start_time: .2f} seconds')"],"metadata":{"id":"ENFkHqye5qMz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"511e5b12-f97b-4fa5-d747-2b17555d43ba","executionInfo":{"status":"ok","timestamp":1682290443834,"user_tz":240,"elapsed":198869,"user":{"displayName":"Jan Darge","userId":"00143714718901829372"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mode:  3\n","LR:  0.1\n","Batch size:  10\n","Torch device selected:  cuda:0\n","\n","Epoch:  1\n","Train set: Average loss: 0.2509, Accuracy: 55551/60000 (93%)\n","Test set: Average loss: 0.1194, Accuracy: 9662/10000 (97%)\n","\n","Epoch:  2\n","Train set: Average loss: 0.1004, Accuracy: 58241/60000 (97%)\n","Test set: Average loss: 0.0896, Accuracy: 9738/10000 (97%)\n","\n","Epoch:  3\n","Train set: Average loss: 0.0637, Accuracy: 58904/60000 (98%)\n","Test set: Average loss: 0.0889, Accuracy: 9760/10000 (98%)\n","\n","Epoch:  4\n","Train set: Average loss: 0.0479, Accuracy: 59115/60000 (99%)\n","Test set: Average loss: 0.0812, Accuracy: 9790/10000 (98%)\n","\n","Epoch:  5\n","Train set: Average loss: 0.0373, Accuracy: 59357/60000 (99%)\n","Test set: Average loss: 0.0824, Accuracy: 9769/10000 (98%)\n","\n","Best accuracy is 97.90\n","\n","Training and evaluation finished\n","\n","Took:  199.02 seconds\n"]}]},{"cell_type":"markdown","source":["#### Previous Log"],"metadata":{"id":"TRoaYETMKdDG"}},{"cell_type":"markdown","source":["```\n","Mode:  3\n","LR:  0.1\n","Batch size:  10\n","Torch device selected:  cuda:0\n","\n","Epoch:  1\n","Train set: Average loss: 0.2522, Accuracy: 55447/60000 (92%)\n","Test set: Average loss: 0.1450, Accuracy: 9562/10000 (96%)\n","\n","Epoch:  2\n","Train set: Average loss: 0.0975, Accuracy: 58285/60000 (97%)\n","Test set: Average loss: 0.0900, Accuracy: 9728/10000 (97%)\n","\n","Epoch:  3\n","Train set: Average loss: 0.0678, Accuracy: 58797/60000 (98%)\n","Test set: Average loss: 0.0811, Accuracy: 9771/10000 (98%)\n","\n","Epoch:  4\n","Train set: Average loss: 0.0473, Accuracy: 59155/60000 (99%)\n","Test set: Average loss: 0.0884, Accuracy: 9769/10000 (98%)\n","\n","Epoch:  5\n","Train set: Average loss: 0.0352, Accuracy: 59353/60000 (99%)\n","Test set: Average loss: 0.0690, Accuracy: 9817/10000 (98%)\n","\n","Best accuracy is 98.17\n","\n","Training and evaluation finished\n","\n","Took:  199.08 seconds\n","```"],"metadata":{"id":"FYosOlu4KeeI"}},{"cell_type":"markdown","source":["#### Reminder"],"metadata":{"id":"IlLBPSmAfEE0"}},{"cell_type":"code","source":["'''\n","\n","NOTE: at this point i didnt realize the instructions said each layer HAS to have >200 neurons \n","\n","def example_of_old_model3(): \n","    \n","    self.m3_fc1 = nn.Linear(28*28, 1024)\n","    self.m3_bn1 = nn.BatchNorm1d(1024)\n","    self.m3_dropout1 = nn.Dropout(p=0.5)\n","    self.m3_fc2 = nn.Linear(1024, 512)\n","    self.m3_bn2 = nn.BatchNorm1d(512)\n","    self.m3_dropout2 = nn.Dropout(p=0.5)\n","    self.m3_fc3 = nn.Linear(512, 256)\n","    self.m3_bn3 = nn.BatchNorm1d(256)\n","    self.m3_dropout3 = nn.Dropout(p=0.5)\n","    self.m3_fc4 = nn.Linear(256, 128)\n","    self.m3_bn4 = nn.BatchNorm1d(128)\n","    self.m3_dropout4 = nn.Dropout(p=0.5)\n","    self.m3_fc5 = nn.Linear(128, 64)\n","    self.m3_bn5 = nn.BatchNorm1d(64)\n","    self.m3_dropout5 = nn.Dropout(p=0.5)\n","    self.m3_fc6 = nn.Linear(64, 32)\n","    self.m3_bn6 = nn.BatchNorm1d(32)\n","    self.m3_dropout6 = nn.Dropout(p=0.5)\n","    self.m3_fc7 = nn.Linear(32, 16)\n","    self.m3_bn7 = nn.BatchNorm1d(16)\n","    self.m3_dropout7 = nn.Dropout(p=0.5)\n","    self.m3_fc8 = nn.Linear(16, 10)\n","\n","        model_3()\n","\n","    def model_3 (self, X):\n","        \n","        X = F.relu(self.m3_bn1(self.m3_fc1(X)))\n","        X = self.m3_dropout1(X)\n","        X = F.relu(self.m3_bn2(self.m3_fc2(X)))\n","        X = self.m3_dropout2(X)\n","        X = F.relu(self.m3_bn3(self.m3_fc3(X)))\n","        X = self.m3_dropout3(X)\n","        X = F.relu(self.m3_bn4(self.m3_fc4(X)))\n","        X = self.m3_dropout4(X)\n","        X = F.relu(self.m3_bn5(self.m3_fc5(X)))\n","        X = self.m3_dropout5(X)\n","        X = F.relu(self.m3_bn6(self.m3_fc6(X)))\n","        X = self.m3_dropout6(X)\n","        X = F.relu(self.m3_bn7(self.m3_fc7(X)))\n","        X = self.m3_dropout7(X)\n","        X = self.m3_fc8(X)\n","    \n","        X = F.log_softmax(X, dim=1)\n","'''\n","\n","# this model requires a lot of epochs"],"metadata":{"id":"oBPzOB_VC6X_","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"00013713-5e4d-454b-bc13-52ad9671cf9d","executionInfo":{"status":"ok","timestamp":1682290443836,"user_tz":240,"elapsed":16,"user":{"displayName":"Jan Darge","userId":"00143714718901829372"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\nNOTE: at this point i didnt realize the instructions said each layer HAS to have >200 neurons \\n\\ndef example_of_old_model3(): \\n    \\n    self.m3_fc1 = nn.Linear(28*28, 1024)\\n    self.m3_bn1 = nn.BatchNorm1d(1024)\\n    self.m3_dropout1 = nn.Dropout(p=0.5)\\n    self.m3_fc2 = nn.Linear(1024, 512)\\n    self.m3_bn2 = nn.BatchNorm1d(512)\\n    self.m3_dropout2 = nn.Dropout(p=0.5)\\n    self.m3_fc3 = nn.Linear(512, 256)\\n    self.m3_bn3 = nn.BatchNorm1d(256)\\n    self.m3_dropout3 = nn.Dropout(p=0.5)\\n    self.m3_fc4 = nn.Linear(256, 128)\\n    self.m3_bn4 = nn.BatchNorm1d(128)\\n    self.m3_dropout4 = nn.Dropout(p=0.5)\\n    self.m3_fc5 = nn.Linear(128, 64)\\n    self.m3_bn5 = nn.BatchNorm1d(64)\\n    self.m3_dropout5 = nn.Dropout(p=0.5)\\n    self.m3_fc6 = nn.Linear(64, 32)\\n    self.m3_bn6 = nn.BatchNorm1d(32)\\n    self.m3_dropout6 = nn.Dropout(p=0.5)\\n    self.m3_fc7 = nn.Linear(32, 16)\\n    self.m3_bn7 = nn.BatchNorm1d(16)\\n    self.m3_dropout7 = nn.Dropout(p=0.5)\\n    self.m3_fc8 = nn.Linear(16, 10)\\n\\n        model_3()\\n\\n    def model_3 (self, X):\\n        \\n        X = F.relu(self.m3_bn1(self.m3_fc1(X)))\\n        X = self.m3_dropout1(X)\\n        X = F.relu(self.m3_bn2(self.m3_fc2(X)))\\n        X = self.m3_dropout2(X)\\n        X = F.relu(self.m3_bn3(self.m3_fc3(X)))\\n        X = self.m3_dropout3(X)\\n        X = F.relu(self.m3_bn4(self.m3_fc4(X)))\\n        X = self.m3_dropout4(X)\\n        X = F.relu(self.m3_bn5(self.m3_fc5(X)))\\n        X = self.m3_dropout5(X)\\n        X = F.relu(self.m3_bn6(self.m3_fc6(X)))\\n        X = self.m3_dropout6(X)\\n        X = F.relu(self.m3_bn7(self.m3_fc7(X)))\\n        X = self.m3_dropout7(X)\\n        X = self.m3_fc8(X)\\n    \\n        X = F.log_softmax(X, dim=1)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["## Write Up"],"metadata":{"id":"V8wqbtS7e6p5"}},{"cell_type":"markdown","source":["### Task 1"],"metadata":{"id":"g-UyLlyMe8cA"}},{"cell_type":"markdown","source":["I was uncertain of my results at first. However, after doing research, my results seem to be a good sign. \n","\n","The best accuracy was 9.80\n","\n","In model 1 we can see nan values, thus all the statistical data we \"collected\" is useless. My research indicates that a model without any activation functions the model won't ve able to learn effectively. As a result, the average loss will become nan due to numerical instability. The model may have become too large or too small, which can cause floating-point values to overflow. \n","\n","That being said, it continued to try and train for as long as I said to and took approx. 174 seconds to complete.\n"],"metadata":{"id":"J-pxIZZcfC4k"}},{"cell_type":"markdown","source":["### Task 2"],"metadata":{"id":"MX3CZIKZe8wW"}},{"cell_type":"markdown","source":["With the inclusion of activation function we can see a stable model being trained. The results speak for themselves. The highest accuracy was around 95%  w/ a loss hovering slightly above 1.5 (this applies for both the training and test set). \n","\n","This loss is still pretty high, indicating that it is making some errors. And, since the loss never really changed, it's possible that it became stuck (plateaued) on a local minimum. It's possible that changing the hyper parameters would lead to a better result, but I wanted to keep it consistent throughout the models to see where the performance varies. \n","\n","So, for a thought experiment let's assume the hyperparameters were not the issue. The data is obviously fine, so poor data quality cannot be the issue. Overfitting is unlikely because both the training and test set have poor performance. With these ideas out of the way, it becomes conclusive that there's poor model capacity. Meaning, the model was not powerful enough to learn the patterns of the data (underfitting). Since it's unedrfitting adding more neurons and more layers should result in a bump in performance.\n","\n","This model took around 180 seconds, which is only a few more than model 1. "],"metadata":{"id":"brJu9VhkfCfn"}},{"cell_type":"markdown","source":["### Task 3"],"metadata":{"id":"-O-gzOq8e8_I"}},{"cell_type":"markdown","source":["Here the model performed exceptionally well. It's hard to say whether or not it's overfitting based on this alone, but I'll explain why this might not be the case. \n","\n","The best accuracy is 99% for training and 98% for test.\n","\n","Here we see a strong accuracy after the first epoch with consistent improvements as it progresses. The loss however drops a lot, which is generally a good thing. But it's important to note that it's really only a good thing if both the training and test set have low loss, this is the case for this particular model. \n","\n","I will note however that the loss is SLIGHTLY larger for the test set. However, much lower on the first epoch. I believe it's because the model is still learning and as a result able to improve quickly. Online I found that this was a good sign, as it points to my model fitting the data appropriately (neither over- or underfitting).\n","\n","This model took a more time than the original two, but not much more, at approxtimately 200 seconds. "],"metadata":{"id":"xchmzQF0fCLS"}},{"cell_type":"markdown","source":["# Part 2"],"metadata":{"id":"DxgRuvwtJouB"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"OQPBHZXkEjWz"}},{"cell_type":"code","source":["from __future__ import print_function\n","import argparse\n","import os\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torch.utils.tensorboard import SummaryWriter\n","#from ConvNet import ConvNet \n","import argparse\n","import numpy as np "],"metadata":{"id":"kC_7f6IfEeeR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"],"metadata":{"id":"w23F5XiAFFUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CNN"],"metadata":{"id":"m2ds7PXnElEk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Af8bXVDEHrh"},"outputs":[],"source":["class ConvNet(nn.Module):\n","    def __init__(self, mode):\n","        super(ConvNet, self).__init__()\n","        \n","        # Define various layers here, such as in the tutorial example\n","        self.conv1   = nn.Conv2d(in_channels=3,  out_channels=10, kernel_size=3)\n","        self.conv2   = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3)\n","        self.conv3   = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3)\n","        self.conv1_1 = nn.Conv2d(in_channels=3,  out_channels=20, kernel_size=3)\n","        self.conv3_1 = nn.Conv2d(in_channels=40, out_channels=40, kernel_size=3)\n","        \n","        \n","        self.fc1_model1 = nn.Linear(360, 100)  # This is first fully connected layer for step 1.\n","        self.fc1_model2 = nn.Linear(1440, 100) # This is first fully connected layer for step 2.\n","        self.fc1_model3 = nn.Linear(640, 100)  # This is first fully connected layer for step 3\n","        \n","        self.fc2 = nn.Linear(100, 10)          # This is 2nd fully connected layer for all models.\n","        \n","        self.fc_model0 = nn.Linear(2250, 100)  # This is for example model.\n","        \n","        \n","        # This will select the forward pass function based on mode for the ConvNet.\n","        # Based on the question, you have 3 modes available for step 1 to 3.\n","        # During creation of each ConvNet model, you will assign one of the valid mode.\n","        # This will fix the forward function (and the network graph) for the entire training/testing\n","        if mode == 1:\n","            self.forward = self.model_1\n","        elif mode == 2:\n","            self.forward = self.model_2\n","        elif mode == 3:\n","            self.forward = self.model_3\n","        elif mode == 0:\n","            self.forward = self.model_0\n","        else: \n","            print(\"Invalid mode \", mode, \"selected. Select between 1-3\")\n","            exit(0)\n","        \n","    \n","    # Example model. Modify this for step 1-3\n","    def model_0(self, X):\n","        # ======================================================================         \n","        \n","        X = F.relu(self.conv1(X))\n","        #print(X.shape)\n","        X = F.max_pool2d(X, kernel_size=2)\n","        #print(X.shape)\n","        \n","        X = torch.flatten(X, start_dim=1)\n","        #print(X.shape)\n","        \n","        X = F.relu(self.fc_model0(X))\n","        X = self.fc2(X)\n","        \n","        return X\n","        \n","    \n","    # Simple CNN. step 1\n","    def model_1(self, X):\n","        # ======================================================================\n","         \n","        # Complete this part as model_0, add one more conv2d layer \n","        # with relu activation followed by maxpool layer.\n","        \n","        X = F.relu(self.conv1(X))\n","        X = F.max_pool2d(X, kernel_size=2)\n","\n","        X = F.relu(self.conv2(X))\n","        X = F.max_pool2d(X, kernel_size=2)\n","\n","        X = torch.flatten(X, start_dim=1)\n","\n","        X = F.relu(self.fc1_model1(X))\n","        X = self.fc2(X)\n","        \n","        return X\n","        \n","\n","    # Increase filters. step 2\n","    def model_2(self, X):\n","        # ======================================================================\n","        \n","        # Complete this part as model_1. Modify in/out channels for conv2d layers.\n","        \n","        X = F.relu(self.conv1_1(X))         # 3 in,  20 out\n","        X = F.max_pool2d(X, kernel_size=2)\n","\n","        X = F.relu(self.conv3(X))           # 20 in, 40 out\n","        X = F.max_pool2d(X, kernel_size=2)\n","\n","        X = torch.flatten(X, start_dim=1)\n","\n","        X = F.relu(self.fc1_model2(X))\n","        X = self.fc2(X)\n","        \n","        return X\n","        \n","\n","    # Large CNN. step 3\n","    def model_3(self, X):\n","        # ======================================================================\n","        \n","        # Complete this part as model_2, add one more conv2d layer \n","        # with relu activation. Do not add maxpool after this new conv2d layer.\n","\n","        X = F.relu(self.conv1_1(X))         # 3 in,  20 out\n","        X = F.max_pool2d(X, kernel_size=2)\n","\n","        X = F.relu(self.conv3(X))           # 20 in,  40 out\n","        X = F.max_pool2d(X, kernel_size=2)\n","\n","        X = F.relu(self.conv3_1(X))         # 40 in,  40 out\n","        \n","        X = torch.flatten(X, start_dim=1)\n","\n","        X = F.relu(self.fc1_model3(X))\n","        X = self.fc2(X)\n","        \n","        return X"]},{"cell_type":"markdown","source":["## Helpers"],"metadata":{"id":"v8tjg6FHEqOg"}},{"cell_type":"markdown","source":["### Train"],"metadata":{"id":"1SyjMkqUEsAe"}},{"cell_type":"code","source":["def train(model, device, train_loader, optimizer, criterion, epoch, batch_size):\n","    '''\n","    Trains the model for an epoch and optimizes it.\n","    model: The model to train. Should already be in correct device.\n","    device: 'cuda' or 'cpu'.\n","    train_loader: dataloader for training samples.\n","    optimizer: optimizer to use for model parameter updates.\n","    criterion: used to compute loss for prediction and target \n","    epoch: Current epoch to train for.\n","    batch_size: Batch size to be used.\n","    '''\n","    \n","    # Set model to train mode before each epoch\n","    model.train()\n","    \n","    # Empty list to store losses \n","    losses = []\n","    correct = 0\n","    \n","    # Iterate over entire training samples (1 epoch)\n","    for batch_idx, batch_sample in enumerate(train_loader):\n","        data, target = batch_sample\n","        \n","        # Push data/label to correct device\n","        data, target = data.to(device), target.to(device)\n","        # print(data.shape)\n","        # print(target.shape)\n","        # exit()\n","        \n","        # Reset optimizer gradients. Avoids grad accumulation (accumulation used in RNN).\n","        optimizer.zero_grad()\n","        \n","        # Do forward pass for current set of data\n","        output = model(data)\n","        \n","        # ======================================================================\n","        # Compute loss based on criterion\n","        loss = criterion(output, target)\n","        \n","        # Computes gradient based on final loss\n","        loss.backward()\n","        \n","        # Store loss\n","        losses.append(loss.item())\n","        \n","        # Optimize model parameters based on learning rate and gradient \n","        optimizer.step()\n","        \n","        # Get predicted index by selecting maximum log-probability\n","        pred = output.argmax(dim=1, keepdim=True)\n","        \n","        # ======================================================================\n","        # Count correct predictions overall \n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","        \n","    train_loss = float(np.mean(losses))\n","    train_acc = correct / ((batch_idx+1) * batch_size)\n","    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n","        float(np.mean(losses)), correct, (batch_idx+1) * batch_size,\n","        100. * correct / ((batch_idx+1) * batch_size)))\n","    return train_loss, train_acc"],"metadata":{"id":"bbJmJ5AvETbp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test"],"metadata":{"id":"_Z7VrllREtdC"}},{"cell_type":"code","source":["def test(model, device, test_loader):\n","    '''\n","    Tests the model.\n","    model: The model to train. Should already be in correct device.\n","    device: 'cuda' or 'cpu'.\n","    test_loader: dataloader for test samples.\n","    '''\n","    \n","    # Set model to eval mode to notify all layers.\n","    model.eval()\n","    \n","    losses = []\n","    correct = 0\n","    \n","    # Set torch.no_grad() to disable gradient computation and backpropagation\n","    with torch.no_grad():\n","        for batch_idx, sample in enumerate(test_loader):\n","            data, target = sample\n","            data, target = data.to(device), target.to(device)\n","            \n","\n","            # Predict for data by doing forward pass\n","            output = model(data)\n","            \n","            # ======================================================================\n","            # Compute loss based on same criterion as training\n","            loss = F.cross_entropy(output, target, reduction='mean')\n","            \n","            # Append loss to overall test loss\n","            losses.append(loss.item())\n","            \n","            # Get predicted index by selecting maximum log-probability\n","            pred = output.argmax(dim=1, keepdim=True)\n","            \n","            # ======================================================================\n","            # Count correct predictions overall \n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss = float(np.mean(losses))\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","\n","    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n","        test_loss, correct, len(test_loader.dataset), accuracy))\n","    \n","    return test_loss, accuracy"],"metadata":{"id":"zFzpOl7WEb_p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main"],"metadata":{"id":"pEgnaoJxEvny"}},{"cell_type":"code","source":["def run_main(FLAGS):\n","    # # Check if cuda is available\n","    # use_cuda = torch.cuda.is_available()\n","    \n","    # # Set proper device based on cuda availability \n","    # device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","    print(\"Torch device selected: \", device)\n","    \n","    # Initialize the model and send to device \n","    model = ConvNet(FLAGS.mode).to(device)\n","    # print(model)\n","    # exit()\n","\n","    # Initialize the criterion for loss computation \n","    criterion = nn.CrossEntropyLoss(reduction='mean')\n","    \n","    # Initialize optimizer type \n","    optimizer = optim.SGD(model.parameters(), lr=FLAGS.learning_rate, weight_decay=1e-7)\n","    \n","    # Create transformations to apply to each data sample \n","    # Can specify variations such as image flip, color flip, random crop, ...\n","    #transform=transforms.Compose([\n","    #    transforms.ToTensor(),\n","    #    transforms.Normalize((0.1307,), (0.3081,))\n","    #    ])\n","    \n","    transform = transforms.Compose(\n","                    [transforms.ToTensor(),\n","                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","     \n","    # Load datasets for training and testing\n","    # Inbuilt datasets available in torchvision (check documentation online)\n","    dataset1 = datasets.CIFAR10('./data/', train=True, download=True,\n","                       transform=transform)\n","    dataset2 = datasets.CIFAR10('./data/', train=False,\n","                       transform=transform)\n","    train_loader = DataLoader(dataset1, batch_size = FLAGS.batch_size, \n","                                shuffle=True, num_workers=4)\n","    test_loader = DataLoader(dataset2, batch_size = FLAGS.batch_size, \n","                                shuffle=False, num_workers=4)\n","    \n","    best_accuracy = 0.0\n","    \n","    # Run training for n_epochs specified in config \n","    for epoch in range(1, FLAGS.num_epochs + 1):\n","        print(\"\\nEpoch: \", epoch)\n","        train_loss, train_accuracy = train(model, device, train_loader,\n","                                            optimizer, criterion, epoch, FLAGS.batch_size)\n","        test_loss, test_accuracy = test(model, device, test_loader)\n","        \n","        if test_accuracy > best_accuracy:\n","            best_accuracy = test_accuracy\n","            \n","    print(\"\\nBest accuracy is {:2.2f}\".format(best_accuracy))\n","    \n","    print(\"\\nTraining and evaluation finished\")"],"metadata":{"id":"_5vf4_BXEZdh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RUN"],"metadata":{"id":"6OmAaxrlExSY"}},{"cell_type":"markdown","source":["### Model 0 (test)"],"metadata":{"id":"gFl1KcvFU0eR"}},{"cell_type":"markdown","source":["#### Code"],"metadata":{"id":"e5adLEjzeN0s"}},{"cell_type":"code","source":["# Set parameters for Sparse Autoencoder\n","parser = argparse.ArgumentParser('CNN Exercise.')\n","parser.add_argument('--mode',\n","                    type=int, default=0,\n","                    help='Select mode between 1-3.')\n","parser.add_argument('--learning_rate',\n","                    type=float, default=0.1,\n","                    help='Initial learning rate.')\n","parser.add_argument('--num_epochs',\n","                    type=int,\n","                    default=10,\n","                    help='Number of epochs to run trainer.')\n","parser.add_argument('--batch_size',\n","                    type=int, default=100,\n","                    help='Batch size. Must divide evenly into the dataset sizes.')\n","parser.add_argument('--log_dir',\n","                    type=str,\n","                    default='logs',\n","                    help='Directory to put logging.')\n","                    \n","FLAGS = None\n","FLAGS, unparsed = parser.parse_known_args()\n","\n","print(\"Mode: \", FLAGS.mode)\n","print(\"LR: \", FLAGS.learning_rate)\n","print(\"Batch size: \", FLAGS.batch_size)\n","\n","start_time = time.time()\n","run_main(FLAGS)\n","end_time = time.time()\n","\n","print(f'\\nTook: {end_time - start_time: .2f} seconds')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"noEdFgOgU2wu","outputId":"4851f7a9-2bed-4790-f1ce-dad51082ff06","executionInfo":{"status":"ok","timestamp":1682291332861,"user_tz":240,"elapsed":168510,"user":{"displayName":"Jan Darge","userId":"00143714718901829372"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mode:  0\n","LR:  0.1\n","Batch size:  100\n","Torch device selected:  cuda:0\n","Files already downloaded and verified\n","\n","Epoch:  1\n","Train set: Average loss: 1.6728, Accuracy: 19844/50000 (40%)\n","Test set: Average loss: 1.4185, Accuracy: 4836/10000 (48%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.3166, Accuracy: 26502/50000 (53%)\n","Test set: Average loss: 1.3185, Accuracy: 5265/10000 (53%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.1821, Accuracy: 29057/50000 (58%)\n","Test set: Average loss: 1.1938, Accuracy: 5763/10000 (58%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.0949, Accuracy: 30685/50000 (61%)\n","Test set: Average loss: 1.1699, Accuracy: 5900/10000 (59%)\n","\n","Epoch:  5\n","Train set: Average loss: 1.0194, Accuracy: 31954/50000 (64%)\n","Test set: Average loss: 1.1829, Accuracy: 5858/10000 (59%)\n","\n","Epoch:  6\n","Train set: Average loss: 0.9529, Accuracy: 33288/50000 (67%)\n","Test set: Average loss: 1.1074, Accuracy: 6112/10000 (61%)\n","\n","Epoch:  7\n","Train set: Average loss: 0.8851, Accuracy: 34339/50000 (69%)\n","Test set: Average loss: 1.1352, Accuracy: 6106/10000 (61%)\n","\n","Epoch:  8\n","Train set: Average loss: 0.8243, Accuracy: 35482/50000 (71%)\n","Test set: Average loss: 1.1464, Accuracy: 6150/10000 (62%)\n","\n","Epoch:  9\n","Train set: Average loss: 0.7605, Accuracy: 36606/50000 (73%)\n","Test set: Average loss: 1.1924, Accuracy: 6076/10000 (61%)\n","\n","Epoch:  10\n","Train set: Average loss: 0.7043, Accuracy: 37591/50000 (75%)\n","Test set: Average loss: 1.1785, Accuracy: 6199/10000 (62%)\n","\n","Best accuracy is 61.99\n","\n","Training and evaluation finished\n","\n","Took:  168.42 seconds\n"]}]},{"cell_type":"markdown","source":["#### Previous Log:"],"metadata":{"id":"zEvlt5w_Zqiq"}},{"cell_type":"markdown","source":["```\n","Mode:  0\n","LR:  0.1\n","Batch size:  100\n","Torch device selected:  cuda:0\n","Files already downloaded and verified\n","\n","Epoch:  1\n","Train set: Average loss: 1.6622, Accuracy: 20273/50000 (41%)\n","Test set: Average loss: 1.3655, Accuracy: 5092/10000 (51%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.3129, Accuracy: 26563/50000 (53%)\n","Test set: Average loss: 1.2876, Accuracy: 5425/10000 (54%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.1766, Accuracy: 29110/50000 (58%)\n","Test set: Average loss: 1.2049, Accuracy: 5663/10000 (57%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.0763, Accuracy: 31024/50000 (62%)\n","Test set: Average loss: 1.1467, Accuracy: 5915/10000 (59%)\n","\n","Epoch:  5\n","Train set: Average loss: 0.9956, Accuracy: 32430/50000 (65%)\n","Test set: Average loss: 1.1731, Accuracy: 5924/10000 (59%)\n","\n","Epoch:  6\n","Train set: Average loss: 0.9207, Accuracy: 33743/50000 (67%)\n","Test set: Average loss: 1.1338, Accuracy: 6076/10000 (61%)\n","\n","Epoch:  7\n","Train set: Average loss: 0.8532, Accuracy: 34919/50000 (70%)\n","Test set: Average loss: 1.1207, Accuracy: 6186/10000 (62%)\n","\n","Epoch:  8\n","Train set: Average loss: 0.7942, Accuracy: 36031/50000 (72%)\n","Test set: Average loss: 1.1431, Accuracy: 6208/10000 (62%)\n","\n","Epoch:  9\n","Train set: Average loss: 0.7302, Accuracy: 37132/50000 (74%)\n","Test set: Average loss: 1.1866, Accuracy: 6142/10000 (61%)\n","\n","Epoch:  10\n","Train set: Average loss: 0.6770, Accuracy: 38075/50000 (76%)\n","Test set: Average loss: 1.1910, Accuracy: 6160/10000 (62%)\n","\n","Best accuracy is 62.08\n","\n","Training and evaluation finished\n","\n","Took:  168.45 seconds\n","```"],"metadata":{"id":"uMl4OMUOZsLC"}},{"cell_type":"markdown","source":["### Model 1"],"metadata":{"id":"OtM9nGr4EzBH"}},{"cell_type":"markdown","source":["#### Code"],"metadata":{"id":"C_lNdSzUeKSg"}},{"cell_type":"code","source":["# Set parameters for Sparse Autoencoder\n","parser = argparse.ArgumentParser('CNN Exercise.')\n","parser.add_argument('--mode',\n","                    type=int, default=1,\n","                    help='Select mode between 1-3.')\n","parser.add_argument('--learning_rate',\n","                    type=float, default=0.1,\n","                    help='Initial learning rate.')\n","parser.add_argument('--num_epochs',\n","                    type=int,\n","                    default=10,\n","                    help='Number of epochs to run trainer.')\n","parser.add_argument('--batch_size',\n","                    type=int, default=100,\n","                    help='Batch size. Must divide evenly into the dataset sizes.')\n","parser.add_argument('--log_dir',\n","                    type=str,\n","                    default='logs',\n","                    help='Directory to put logging.')\n","                    \n","FLAGS = None\n","FLAGS, unparsed = parser.parse_known_args()\n","\n","print(\"Mode: \", FLAGS.mode)\n","print(\"LR: \", FLAGS.learning_rate)\n","print(\"Batch size: \", FLAGS.batch_size)\n","\n","start_time = time.time()\n","run_main(FLAGS)\n","end_time = time.time()\n","\n","print(f'\\nTook: {end_time - start_time: .2f} seconds')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ntvbehcjEWDq","outputId":"052269c9-012f-4cc5-9113-1c25f38c477d","executionInfo":{"status":"ok","timestamp":1682290805135,"user_tz":240,"elapsed":168532,"user":{"displayName":"Jan Darge","userId":"00143714718901829372"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mode:  1\n","LR:  0.1\n","Batch size:  100\n","Torch device selected:  cuda:0\n","Files already downloaded and verified\n","\n","Epoch:  1\n","Train set: Average loss: 1.8979, Accuracy: 15449/50000 (31%)\n","Test set: Average loss: 1.6066, Accuracy: 4233/10000 (42%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.5529, Accuracy: 22113/50000 (44%)\n","Test set: Average loss: 1.4281, Accuracy: 4809/10000 (48%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.3955, Accuracy: 25090/50000 (50%)\n","Test set: Average loss: 1.3451, Accuracy: 5150/10000 (52%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.2765, Accuracy: 27410/50000 (55%)\n","Test set: Average loss: 1.2252, Accuracy: 5714/10000 (57%)\n","\n","Epoch:  5\n","Train set: Average loss: 1.1862, Accuracy: 29132/50000 (58%)\n","Test set: Average loss: 1.1663, Accuracy: 5844/10000 (58%)\n","\n","Epoch:  6\n","Train set: Average loss: 1.1231, Accuracy: 30332/50000 (61%)\n","Test set: Average loss: 1.1686, Accuracy: 5936/10000 (59%)\n","\n","Epoch:  7\n","Train set: Average loss: 1.0707, Accuracy: 31198/50000 (62%)\n","Test set: Average loss: 1.1925, Accuracy: 5837/10000 (58%)\n","\n","Epoch:  8\n","Train set: Average loss: 1.0195, Accuracy: 32023/50000 (64%)\n","Test set: Average loss: 1.1216, Accuracy: 6143/10000 (61%)\n","\n","Epoch:  9\n","Train set: Average loss: 0.9815, Accuracy: 32801/50000 (66%)\n","Test set: Average loss: 1.1182, Accuracy: 6094/10000 (61%)\n","\n","Epoch:  10\n","Train set: Average loss: 0.9436, Accuracy: 33508/50000 (67%)\n","Test set: Average loss: 1.0800, Accuracy: 6280/10000 (63%)\n","\n","Best accuracy is 62.80\n","\n","Training and evaluation finished\n","\n","Took:  168.71 seconds\n"]}]},{"cell_type":"markdown","source":["#### Previous Log:"],"metadata":{"id":"bEMorivnZd23"}},{"cell_type":"markdown","source":["```\n","Mode:  1\n","LR:  0.1\n","Batch size:  100\n","Torch device selected:  cuda:0\n","Files already downloaded and verified\n","\n","Epoch:  1\n","Train set: Average loss: 1.8628, Accuracy: 16244/50000 (32%)\n","Test set: Average loss: 1.5160, Accuracy: 4571/10000 (46%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.4774, Accuracy: 23481/50000 (47%)\n","Test set: Average loss: 1.3735, Accuracy: 5063/10000 (51%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.3291, Accuracy: 26272/50000 (53%)\n","Test set: Average loss: 1.2637, Accuracy: 5486/10000 (55%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.2292, Accuracy: 28281/50000 (57%)\n","Test set: Average loss: 1.2102, Accuracy: 5721/10000 (57%)\n","\n","Epoch:  5\n","Train set: Average loss: 1.1502, Accuracy: 29841/50000 (60%)\n","Test set: Average loss: 1.1888, Accuracy: 5815/10000 (58%)\n","\n","Epoch:  6\n","Train set: Average loss: 1.0876, Accuracy: 30897/50000 (62%)\n","Test set: Average loss: 1.1159, Accuracy: 6049/10000 (60%)\n","\n","Epoch:  7\n","Train set: Average loss: 1.0319, Accuracy: 31894/50000 (64%)\n","Test set: Average loss: 1.0957, Accuracy: 6188/10000 (62%)\n","\n","Epoch:  8\n","Train set: Average loss: 0.9804, Accuracy: 32788/50000 (66%)\n","Test set: Average loss: 1.1771, Accuracy: 6058/10000 (61%)\n","\n","Epoch:  9\n","Train set: Average loss: 0.9422, Accuracy: 33577/50000 (67%)\n","Test set: Average loss: 1.0351, Accuracy: 6402/10000 (64%)\n","\n","Epoch:  10\n","Train set: Average loss: 0.9020, Accuracy: 34253/50000 (69%)\n","Test set: Average loss: 1.0265, Accuracy: 6474/10000 (65%)\n","\n","Best accuracy is 64.74\n","\n","Training and evaluation finished\n","```"],"metadata":{"id":"1dEwvBoTZgqU"}},{"cell_type":"markdown","source":["### Model 2"],"metadata":{"id":"twrXxg2bZicc"}},{"cell_type":"markdown","source":["#### Code"],"metadata":{"id":"28s2vajZeH-L"}},{"cell_type":"code","source":["# Set parameters for Sparse Autoencoder\n","parser = argparse.ArgumentParser('CNN Exercise.')\n","parser.add_argument('--mode',\n","                    type=int, default=2,\n","                    help='Select mode between 1-3.')\n","parser.add_argument('--learning_rate',\n","                    type=float, default=0.1,\n","                    help='Initial learning rate.')\n","parser.add_argument('--num_epochs',\n","                    type=int,\n","                    default=10,\n","                    help='Number of epochs to run trainer.')\n","parser.add_argument('--batch_size',\n","                    type=int, default=100,\n","                    help='Batch size. Must divide evenly into the dataset sizes.')\n","parser.add_argument('--log_dir',\n","                    type=str,\n","                    default='logs',\n","                    help='Directory to put logging.')\n","                    \n","FLAGS = None\n","FLAGS, unparsed = parser.parse_known_args()\n","\n","print(\"Mode: \", FLAGS.mode)\n","print(\"LR: \", FLAGS.learning_rate)\n","print(\"Batch size: \", FLAGS.batch_size)\n","\n","start_time = time.time()\n","run_main(FLAGS)\n","end_time = time.time()\n","\n","print(f'\\nTook: {end_time - start_time: .2f} seconds')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4STddovaZh47","outputId":"ab389162-aca6-4c20-fac9-a09862c9a458","executionInfo":{"status":"ok","timestamp":1682290992317,"user_tz":240,"elapsed":187187,"user":{"displayName":"Jan Darge","userId":"00143714718901829372"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mode:  2\n","LR:  0.1\n","Batch size:  100\n","Torch device selected:  cuda:0\n","Files already downloaded and verified\n","\n","Epoch:  1\n","Train set: Average loss: 1.7663, Accuracy: 18026/50000 (36%)\n","Test set: Average loss: 1.4331, Accuracy: 4908/10000 (49%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.3423, Accuracy: 26198/50000 (52%)\n","Test set: Average loss: 1.1926, Accuracy: 5750/10000 (58%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.1489, Accuracy: 29709/50000 (59%)\n","Test set: Average loss: 1.1131, Accuracy: 6072/10000 (61%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.0236, Accuracy: 31979/50000 (64%)\n","Test set: Average loss: 1.0527, Accuracy: 6351/10000 (64%)\n","\n","Epoch:  5\n","Train set: Average loss: 0.9259, Accuracy: 33794/50000 (68%)\n","Test set: Average loss: 0.9988, Accuracy: 6562/10000 (66%)\n","\n","Epoch:  6\n","Train set: Average loss: 0.8472, Accuracy: 35230/50000 (70%)\n","Test set: Average loss: 0.9607, Accuracy: 6686/10000 (67%)\n","\n","Epoch:  7\n","Train set: Average loss: 0.7784, Accuracy: 36401/50000 (73%)\n","Test set: Average loss: 0.9034, Accuracy: 6885/10000 (69%)\n","\n","Epoch:  8\n","Train set: Average loss: 0.7068, Accuracy: 37577/50000 (75%)\n","Test set: Average loss: 0.9229, Accuracy: 6879/10000 (69%)\n","\n","Epoch:  9\n","Train set: Average loss: 0.6485, Accuracy: 38606/50000 (77%)\n","Test set: Average loss: 0.9005, Accuracy: 6960/10000 (70%)\n","\n","Epoch:  10\n","Train set: Average loss: 0.5816, Accuracy: 39816/50000 (80%)\n","Test set: Average loss: 0.9139, Accuracy: 7034/10000 (70%)\n","\n","Best accuracy is 70.34\n","\n","Training and evaluation finished\n","\n","Took:  187.15 seconds\n"]}]},{"cell_type":"markdown","source":["#### Previous Log:"],"metadata":{"id":"M5V6CJkBZt5J"}},{"cell_type":"markdown","source":["```\n","Mode:  2\n","LR:  0.1\n","Batch size:  100\n","Torch device selected:  cuda:0\n","Files already downloaded and verified\n","\n","Epoch:  1\n","Train set: Average loss: 1.7852, Accuracy: 17450/50000 (35%)\n","Test set: Average loss: 1.4772, Accuracy: 4601/10000 (46%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.3660, Accuracy: 25537/50000 (51%)\n","Test set: Average loss: 1.2365, Accuracy: 5580/10000 (56%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.1770, Accuracy: 29141/50000 (58%)\n","Test set: Average loss: 1.1384, Accuracy: 5978/10000 (60%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.0418, Accuracy: 31632/50000 (63%)\n","Test set: Average loss: 1.0319, Accuracy: 6355/10000 (64%)\n","\n","Epoch:  5\n","Train set: Average loss: 0.9432, Accuracy: 33396/50000 (67%)\n","Test set: Average loss: 0.9678, Accuracy: 6669/10000 (67%)\n","\n","Epoch:  6\n","Train set: Average loss: 0.8617, Accuracy: 34971/50000 (70%)\n","Test set: Average loss: 0.9731, Accuracy: 6622/10000 (66%)\n","\n","Epoch:  7\n","Train set: Average loss: 0.7902, Accuracy: 36219/50000 (72%)\n","Test set: Average loss: 0.9296, Accuracy: 6804/10000 (68%)\n","\n","Epoch:  8\n","Train set: Average loss: 0.7234, Accuracy: 37331/50000 (75%)\n","Test set: Average loss: 0.9737, Accuracy: 6741/10000 (67%)\n","\n","Epoch:  9\n","Train set: Average loss: 0.6577, Accuracy: 38578/50000 (77%)\n","Test set: Average loss: 0.9021, Accuracy: 6961/10000 (70%)\n","\n","Epoch:  10\n","Train set: Average loss: 0.5965, Accuracy: 39515/50000 (79%)\n","Test set: Average loss: 0.9502, Accuracy: 6878/10000 (69%)\n","\n","Best accuracy is 69.61\n","\n","Training and evaluation finished\n","\n","Took:  174.79 seconds\n","```"],"metadata":{"id":"QXuHcS4bZvpA"}},{"cell_type":"markdown","source":["### Model 3"],"metadata":{"id":"BvikxfwwZwbE"}},{"cell_type":"markdown","source":["#### Code"],"metadata":{"id":"GlTdqqAieewX"}},{"cell_type":"code","source":["# Set parameters for Sparse Autoencoder\n","parser = argparse.ArgumentParser('CNN Exercise.')\n","parser.add_argument('--mode',\n","                    type=int, default=3,\n","                    help='Select mode between 1-3.')\n","parser.add_argument('--learning_rate',\n","                    type=float, default=0.1,\n","                    help='Initial learning rate.')\n","parser.add_argument('--num_epochs',\n","                    type=int,\n","                    default=10,\n","                    help='Number of epochs to run trainer.')\n","parser.add_argument('--batch_size',\n","                    type=int, default=100,\n","                    help='Batch size. Must divide evenly into the dataset sizes.')\n","parser.add_argument('--log_dir',\n","                    type=str,\n","                    default='logs',\n","                    help='Directory to put logging.')\n","                    \n","FLAGS = None\n","FLAGS, unparsed = parser.parse_known_args()\n","\n","print(\"Mode: \", FLAGS.mode)\n","print(\"LR: \", FLAGS.learning_rate)\n","print(\"Batch size: \", FLAGS.batch_size)\n","\n","start_time = time.time()\n","run_main(FLAGS)\n","end_time = time.time()\n","\n","print(f'\\nTook: {end_time - start_time: .2f} seconds')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LWljQGd_ZxcW","outputId":"27c82362-8d61-4846-f58b-47d26474b2a2","executionInfo":{"status":"ok","timestamp":1682291163964,"user_tz":240,"elapsed":171652,"user":{"displayName":"Jan Darge","userId":"00143714718901829372"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mode:  3\n","LR:  0.1\n","Batch size:  100\n","Torch device selected:  cuda:0\n","Files already downloaded and verified\n","\n","Epoch:  1\n","Train set: Average loss: 1.9553, Accuracy: 14138/50000 (28%)\n","Test set: Average loss: 1.6513, Accuracy: 4064/10000 (41%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.4948, Accuracy: 22865/50000 (46%)\n","Test set: Average loss: 1.4566, Accuracy: 4808/10000 (48%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.3139, Accuracy: 26374/50000 (53%)\n","Test set: Average loss: 1.2275, Accuracy: 5598/10000 (56%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.1731, Accuracy: 28984/50000 (58%)\n","Test set: Average loss: 1.0988, Accuracy: 6108/10000 (61%)\n","\n","Epoch:  5\n","Train set: Average loss: 1.0661, Accuracy: 31192/50000 (62%)\n","Test set: Average loss: 1.0412, Accuracy: 6322/10000 (63%)\n","\n","Epoch:  6\n","Train set: Average loss: 0.9807, Accuracy: 32553/50000 (65%)\n","Test set: Average loss: 0.9792, Accuracy: 6555/10000 (66%)\n","\n","Epoch:  7\n","Train set: Average loss: 0.9015, Accuracy: 34104/50000 (68%)\n","Test set: Average loss: 1.0267, Accuracy: 6473/10000 (65%)\n","\n","Epoch:  8\n","Train set: Average loss: 0.8381, Accuracy: 35189/50000 (70%)\n","Test set: Average loss: 0.9091, Accuracy: 6865/10000 (69%)\n","\n","Epoch:  9\n","Train set: Average loss: 0.7845, Accuracy: 36202/50000 (72%)\n","Test set: Average loss: 0.9300, Accuracy: 6817/10000 (68%)\n","\n","Epoch:  10\n","Train set: Average loss: 0.7394, Accuracy: 36941/50000 (74%)\n","Test set: Average loss: 0.9158, Accuracy: 6800/10000 (68%)\n","\n","Best accuracy is 68.65\n","\n","Training and evaluation finished\n","\n","Took:  171.83 seconds\n"]}]},{"cell_type":"markdown","source":["#### Previous Log:"],"metadata":{"id":"_zqAKNRWZyGI"}},{"cell_type":"markdown","source":["```\n","Mode:  3\n","LR:  0.1\n","Batch size:  100\n","Torch device selected:  cuda:0\n","Files already downloaded and verified\n","\n","Epoch:  1\n","Train set: Average loss: 1.9598, Accuracy: 13665/50000 (27%)\n","Test set: Average loss: 1.5871, Accuracy: 4219/10000 (42%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.4972, Accuracy: 22794/50000 (46%)\n","Test set: Average loss: 1.3544, Accuracy: 5039/10000 (50%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.3026, Accuracy: 26884/50000 (54%)\n","Test set: Average loss: 1.2759, Accuracy: 5420/10000 (54%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.1651, Accuracy: 29222/50000 (58%)\n","Test set: Average loss: 1.2663, Accuracy: 5495/10000 (55%)\n","\n","Epoch:  5\n","Train set: Average loss: 1.0573, Accuracy: 31295/50000 (63%)\n","Test set: Average loss: 1.0332, Accuracy: 6350/10000 (64%)\n","\n","Epoch:  6\n","Train set: Average loss: 0.9758, Accuracy: 32919/50000 (66%)\n","Test set: Average loss: 1.0176, Accuracy: 6381/10000 (64%)\n","\n","Epoch:  7\n","Train set: Average loss: 0.9032, Accuracy: 34086/50000 (68%)\n","Test set: Average loss: 0.9408, Accuracy: 6716/10000 (67%)\n","\n","Epoch:  8\n","Train set: Average loss: 0.8414, Accuracy: 35149/50000 (70%)\n","Test set: Average loss: 0.9086, Accuracy: 6830/10000 (68%)\n","\n","Epoch:  9\n","Train set: Average loss: 0.7875, Accuracy: 36171/50000 (72%)\n","Test set: Average loss: 0.8954, Accuracy: 6905/10000 (69%)\n","\n","Epoch:  10\n","Train set: Average loss: 0.7339, Accuracy: 37091/50000 (74%)\n","Test set: Average loss: 0.9393, Accuracy: 6731/10000 (67%)\n","\n","Best accuracy is 69.05\n","\n","Training and evaluation finished\n","\n","Took:  174.89 seconds\n","```"],"metadata":{"id":"fkW8SO3ZZ0_S"}},{"cell_type":"markdown","source":["## Write Up"],"metadata":{"id":"M9QvZ0N9Z1iu"}},{"cell_type":"markdown","source":["### Task 4"],"metadata":{"id":"nNRD0CnAZ3UA"}},{"cell_type":"markdown","source":["* The best accuracy is 69% for training and 65 for test.\n","* Underfitting\n","* ~ 169 seconds\n","\n","This model appears to be underfitting for a few reasons. Both the training and testing set have high losses with low accuracies, that are close to the same performance. \n","\n","This model definitely needs more neurons and layers as it is not complex enough to learn the data properly.\n"],"metadata":{"id":"-nAt9a0IZ-P4"}},{"cell_type":"markdown","source":["### Task 5"],"metadata":{"id":"0GasOrepZ3nW"}},{"cell_type":"markdown","source":["* The best accuracy is 79% for training and 69% for test.\n","* Substantial overfitting\n","* ~ 174 seconds\n","\n","This model is very obviously overfitting the data. We can see by the tenth epoch that the loss is pretty low and the accuracy is moderately high for the training set. However, the test set show a still high loss rate and a pretty low accuracy. \n","\n","Because this is the case, it's obvious that the model is overfitting quite a bit. \n","\n","The total time required to train this model was slightly more than Task 4, but that's expected since we increased the complexity quite a bit."],"metadata":{"id":"ZuanqDGuZ90k"}},{"cell_type":"markdown","source":["### Task 6"],"metadata":{"id":"ScEt7lGUZ3_1"}},{"cell_type":"markdown","source":["* The best accuracy is 74% (epoch 10) for training and 69% (epoch 9) for test.\n","* Overfitting\n","* ~ 174 seconds\n","\n","The accuracies are closer together for both the training and test set, however there is still a gap between the two sets loss amounts. Note, it started off decent, maybe even promising, but as the epochs progressed the issues became clear as the values quickly diverged from eachother. \n","\n","That being said, it's not overfitting nearly as bad as the one from Task 5. Additional layers and neurons should be explored at this point, as well as hyperparameter tuning.\n","\n","The total time to train this model was equivalent to Task 5, which is a bit suprising since the complexity of the model was amped up a little bit. I have no idea why this would be the case."],"metadata":{"id":"8VbZxM1RZ9hf"}},{"cell_type":"code","source":[],"metadata":{"id":"cORpX49EWz6w"},"execution_count":null,"outputs":[]}]}